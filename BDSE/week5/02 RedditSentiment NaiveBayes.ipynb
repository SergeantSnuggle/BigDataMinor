{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.learndatasci.com/tutorials/predicting-reddit-news-sentiment-naive-bayes-text-classifiers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# Prevent future/deprecation warnings from showing in output\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are basic imports used across the entire notebook, and are usually imported in every data science project. The more specific imports from sklearn and other libraries will be brought up when we use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset\n",
    "\n",
    "First let's load the dataset that we created in the last article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>label</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GOP voters more likely to choose candidates wh...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. Rep. John Yarmuth says Louisville Kroger ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trump Comes Face to Face With His Nightmare: A...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump ‘Most Consequential’ President Since Lin...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'We're heading north!' Migrants nix offer to s...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline label  Unnamed: 2\n",
       "0  GOP voters more likely to choose candidates wh...     0         NaN\n",
       "1  U.S. Rep. John Yarmuth says Louisville Kroger ...    -1         NaN\n",
       "2  Trump Comes Face to Face With His Nightmare: A...     0         NaN\n",
       "3  Trump ‘Most Consequential’ President Since Lin...     0         NaN\n",
       "4  'We're heading north!' Migrants nix offer to s...     0         NaN"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('reddit_headlines_labels.csv', encoding='utf-8')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "headline       object\n",
       "label          object\n",
       "Unnamed: 2    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1                                             12515\n",
       "0                                              11152\n",
       "1                                               4743\n",
       "label                                             30\n",
       " Beto O'Rourke calls for Americans to unify        1\n",
       " draws link to stickers on suspect's van           1\n",
       " House a ‘complete dogfight’                       1\n",
       " Trump Blames Lack of Guns                         1\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the dataset in a dataframe, let's remove the neutral (0) headlines labels so we can focus on only classifying positive or negative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNeg =( \n",
    "    df\n",
    "    .loc[lambda df: df['label']== '-1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPos =( \n",
    "    df\n",
    "    .loc[lambda df: df['label']== '1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.concat([dfNeg, dfPos],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    12515\n",
       "1      4743\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Our dataframe now only contains positive and negative examples, and we've confirmed again that we have more negatives than positives.\n",
    "\n",
    "Let's move into featurization of the headlines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform lines into Features (= columns)\n",
    "\n",
    "In order to train our classifier, we need to transform our lines of words into numbers, since algorithms only know how to work with numbers.\n",
    "\n",
    "To do this transformation, we're going to use `CountVectorizer` from sklearn. This is a very straightforward class for converting words into features.\n",
    "\n",
    "Unlike in the last tutorial where we manually tokenized and lowercased the text, `CountVectorizer` will handle this step for us. All we need to do is pass it the headlines.\n",
    "\n",
    "Let's work with a tiny example to show how vectorizing words into numbers works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 2],\n",
       "       [0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "s1 = \"Senate panel moving ahead, with caution, with Mueller bill despite McConnell opposition\"\n",
    "s2 = \"Bill protecting Robert Mueller to get vote despite McConnell opposition\"\n",
    "\n",
    "vect = CountVectorizer(binary=False)\n",
    "X = vect.fit_transform([s1, s2])  # fit_tranform  or just transform \n",
    "# fit_transform all the newwords are accounted for ==> TRAIN set\n",
    "# transform  only the words already met ( in the TRAIN)  are accounted for ==> TEST set \n",
    "\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we've done here is take two headlines about a similar topic and vectorized them.\n",
    "\n",
    "`vect` is set up with default params to tokenize and lowercase words. On top of that, we have set `binary=True` so we get an output of 0 (word doesn't exist in that sentence) or 1 (word exists in that sentence).\n",
    "\n",
    "`vect` builds a vocabulary from all the words it sees in all the text you give it, then assigns a 0 or 1 if that word exists in the current sentence. To see this more clearly, let's check out the feature names mapped to the first sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'ahead'),\n",
       " (1, 'bill'),\n",
       " (1, 'caution'),\n",
       " (1, 'despite'),\n",
       " (0, 'get'),\n",
       " (1, 'mcconnell'),\n",
       " (1, 'moving'),\n",
       " (1, 'mueller'),\n",
       " (1, 'opposition'),\n",
       " (1, 'panel'),\n",
       " (0, 'protecting'),\n",
       " (0, 'robert'),\n",
       " (1, 'senate'),\n",
       " (0, 'to'),\n",
       " (0, 'vote'),\n",
       " (2, 'with')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(X.toarray()[0], vect.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the vectorization mapping of the first sentence. You can see that there's a 1 mapped to 'ahead' because 'ahead' shows up in `s1`.  But if we look at `s2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'ahead'),\n",
       " (1, 'bill'),\n",
       " (0, 'caution'),\n",
       " (1, 'despite'),\n",
       " (1, 'get'),\n",
       " (1, 'mcconnell'),\n",
       " (0, 'moving'),\n",
       " (1, 'mueller'),\n",
       " (1, 'opposition'),\n",
       " (0, 'panel'),\n",
       " (1, 'protecting'),\n",
       " (1, 'robert'),\n",
       " (0, 'senate'),\n",
       " (1, 'to'),\n",
       " (1, 'vote'),\n",
       " (0, 'with')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(X.toarray()[1], vect.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a 0 at 'ahead' since that word doesn't show up in `s2`. But notice that each row contains **every** word seen so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing for Training\n",
    "\n",
    "Before training, and even vectorizing, let's split our data into training and testing sets. It's important to do this before doing anything with the data so we have a fresh test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df[\"headline\"]\n",
    "y = df[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our test size is 0.2, or 20%. This means that `X_test` and `y_test` contains 20% of our data which we reserve for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now fit the vectorizer on the training set only and perform the vectorization. \n",
    "\n",
    "Just to reiterate, it's important to not fit the vectorizer on all of the data since we want a clean test set for evaluating performance. Fitting the vectorizer on everything would result in *data leakage*, causing unreliable results since the vectorizer shouldn't know about future data.\n",
    "\n",
    "We can fit the vectorizer and transform `X_train` in one step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "## VERY HANDY FOR NOT THAT FANCY COMPUTERS, SETTING MAX_FEATURES IN THE COUnTVECTORIZER\n",
    "\n",
    "vect = CountVectorizer(max_features=1000, binary=False)\n",
    "\n",
    "## NOTE AGAIN THE USE OF fit_transform FOR THE TRAIN SET\n",
    "## NOTE AGAIN THE USE OF     transform FOR THE TEST SET\n",
    "\n",
    "X_train_vect = vect.fit_transform(X_train)\n",
    "X_test_vect = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`X_train_vect` is now transformed into the right format to give to the Naive Bayes model, but let's first look into balancing the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing the data\n",
    "\n",
    "It seems that there may be a lot more negative headlines than positive headlines (hmm), and so we have a lot more negative labels than positive labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df[\"label\"].value_counts()\n",
    "print(counts)\n",
    "\n",
    "print(\"\\nPredicting only -1 = {:.2f}% accuracy\".format(counts[0] / sum(counts) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from above, we have slightly more negatives than positives, making our dataset slightly imbalanced.\n",
    "\n",
    "By calculating if our model only chose to predict -1, the larger class, we would get a ~72,5% accuracy. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our first algorithm, we're going to use the extremely fast and versatile Naive Bayes model.\n",
    "\n",
    "Let's instantiate one from sklearn and fit it to our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9245255685933652"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = MultinomialNB()\n",
    "\n",
    "model.fit(X_train_vect, y_train)\n",
    "\n",
    "model.score(X_train_vect, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes has successfully fit all of our training data and is ready to make predictions. You'll notice that we have a score of ~92%. This is the *fit* score, and not the actual accuracy score. You'll see next that we need to use our test set in order to get a good estimate of accuracy.\n",
    "\n",
    "Let's vectorize the test set, then use that test set to predict if each test headline is either positive or negative. Since we're avoiding any data leakage, we are only transforming, not refitting. And we won't be using SMOTE to oversample either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['-1', '-1', '-1', ..., '-1', '-1', '-1'], dtype='<U2')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test_vect)\n",
    "y_prob =  model.predict_proba(X_test_vect)[::,1]\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`y_pred` now contains a prediction for every row of the test set. With this prediction result, we can pass it into an sklearn metric with the true labels to get an accuracy score, F1 score, and generate a confusion matrix: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 91.31%\n",
      "Confusion Matrix:\n",
      " [[2351  166]\n",
      " [ 134  801]]\n",
      "AUC: \n",
      " 0.9702540372525649\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix,roc_auc_score\n",
    "\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy_score(y_test, y_pred) * 100))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"AUC: \\n\", roc_auc_score(y_test, y_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Classification Algorithms in scikit-learn\n",
    "\n",
    "As you can see Naive Bayes performed pretty well, so let’s experiment with other classifiers.\n",
    "\n",
    "We'll use the same shuffle splitting as before, but now we'll run several types of models in each loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "models = [\n",
    "    BernoulliNB(),\n",
    "    LogisticRegression(),\n",
    "    RandomForestClassifier()\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Init a dictionary for storing results of each run for each model\n",
    "results = {\n",
    "    model.__class__.__name__: {\n",
    "        'accuracy': [], \n",
    "        'confusion_matrix': [],\n",
    "        'auc': []\n",
    "    } for model in models\n",
    "}\n",
    "\n",
    "\n",
    "X_train_vect = vect.fit_transform(X_train)\n",
    "X_test_vect = vect.transform(X_test)    \n",
    "\n",
    "for model in models:\n",
    "    model.fit(X_train_vect, y_train)\n",
    "    y_pred = model.predict(X_test_vect)\n",
    "    y_prob =  model.predict_proba(X_test_vect)[::,1]\n",
    "        \n",
    "    acc = accuracy_score(y_test, y_pred)   \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    auc= roc_auc_score(y_test, y_prob)\n",
    "        \n",
    "    results[model.__class__.__name__]['accuracy'].append(acc)\n",
    "    results[model.__class__.__name__]['confusion_matrix'].append(cm) \n",
    "    results[model.__class__.__name__]['auc'].append(auc)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a bunch of accuracy scores and confusion matrices stored for each model. Let's average these together to get average scores across models and folds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slashes = '-' * 30\n",
    "for model, d in results.items():\n",
    "    avg_acc = sum(d['accuracy']) / len(d['accuracy']) * 100\n",
    "    avg_cm = sum(d['confusion_matrix']) / len(d['confusion_matrix'])\n",
    "    avg_auc= sum(d['auc']) / len(d['auc']) \n",
    "    \n",
    "    s = f\"\"\"{model}\\n{slashes}\n",
    "        Accuracy: {avg_acc:.2f}%\n",
    "           Confusion Matrix: \n",
    "        \\n{avg_cm}\n",
    "        \\n{avg_auc}\n",
    "        \"\"\"\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
